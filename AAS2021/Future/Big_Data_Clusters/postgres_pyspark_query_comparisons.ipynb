{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook highlights on-going exploratory work comparing Postgresql query performance with PySpark SQL queries using the SDSS MaNGA DR15 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'name': 'brian-query-benchmarks-2', 'executorMemory': '36G', 'numExecutors': 15, 'executorCores': 10, 'conf': {'spark.yarn.appMasterEnv.PYSPARK_PYTHON': 'python3'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>21</td><td>application_1609885494103_0025</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://sparkhead-0.sparkhead-svc:8090/proxy/application_1609885494103_0025/\">Link</a></td><td><a target=\"_blank\" href=\"https://storage-0-0.storage-0-svc.filedb.svc.cluster.local:8044/node/containerlogs/container_1609885494103_0025_01_000001/root\">Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"name\": \"brian-query-benchmarks-2\", \"executorMemory\": \"36G\", \"numExecutors\": 15, \"executorCores\": 10,\n",
    " \"conf\": {\"spark.yarn.appMasterEnv.PYSPARK_PYTHON\":\"python3\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>22</td><td>application_1609885494103_0026</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://sparkhead-0.sparkhead-svc:8090/proxy/application_1609885494103_0026/\">Link</a></td><td><a target=\"_blank\" href=\"https://storage-0-0.storage-0-svc.filedb.svc.cluster.local:8044/node/containerlogs/container_1609885494103_0026_01_000001/root\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a simple timer context manager\n",
    "class Timer(object):\n",
    "    def __enter__(self):\n",
    "        self.start_time = time.perf_counter()\n",
    "        return self\n",
    "   \n",
    "    def __exit__(self, *exc_info):\n",
    "        self.end_time = time.perf_counter()\n",
    "        elapsed = self.end_time-self.start_time\n",
    "        print('Elapsed time [sec]:', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time [sec]: 47.467089012265205"
     ]
    }
   ],
   "source": [
    "# creates a temporary \"database\" table\n",
    "with Timer():\n",
    "    drpall = spark.read.parquet('hdfs:///manga/brian-test/dr15/v2_4_3/drpall')\n",
    "    drpall.createOrReplaceTempView('drpall')\n",
    "    dapall = spark.read.parquet('hdfs:///manga/brian-test/dr15/v2_4_3/dapall')\n",
    "    dapall.createOrReplaceTempView('dapall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time [sec]: 2.9763775691390038"
     ]
    }
   ],
   "source": [
    "# creates a temporary \"database\" table for the DAP maps\n",
    "with Timer():\n",
    "    maps = spark.read.parquet('hdfs:///manga/brian-test/dr15/v2_4_3/maps')\n",
    "    maps.createOrReplaceTempView('maps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time [sec]: 1.4119328930974007"
     ]
    }
   ],
   "source": [
    "# creates a temporary table for the DRP cubes\n",
    "with Timer():\n",
    "    cubes = spark.read.parquet('hdfs:///manga/arik-test/dr15/v2_4_3/logcube_voxel')\n",
    "    cubes.createOrReplaceTempView('cubes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Query 1\n",
    "Select all galaxies with an H-alpha flux value > 5 in more than 20% of \"good\" spaxels, with \"good\" defined as a measured DAP value != -1\n",
    "\n",
    "### Raw SQL \n",
    "\n",
    "SELECT anon_1.mangadatadb_cube_mangaid, anon_1.mangadatadb_cube_plate, concat(anon_1.mangadatadb_cube_plate, '-', anon_1.mangadatadb_ifudesign_name) AS plateifu, anon_1.mangadatadb_ifudesign_name\n",
    "FROM (SELECT mangadatadb.cube.mangaid AS mangadatadb_cube_mangaid, mangadatadb.cube.plate AS mangadatadb_cube_plate, concat(mangadatadb.cube.plate, '-', mangadatadb.ifudesign.name) AS plateifu, mangadatadb.ifudesign.name AS mangadatadb_ifudesign_name, mangadapdb.cleanspaxelprop7.emline_gflux_ha_6564 AS mangadapdb_cleanspaxelprop7_emline_gflux_ha_6564, mangadapdb.cleanspaxelprop7.x AS mangadapdb_cleanspaxelprop7_x, mangadapdb.cleanspaxelprop7.y AS mangadapdb_cleanspaxelprop7_y\n",
    "FROM mangadatadb.cube JOIN mangadatadb.ifudesign ON mangadatadb.ifudesign.pk = mangadatadb.cube.ifudesign_pk JOIN mangadapdb.file ON mangadatadb.cube.pk = mangadapdb.file.cube_pk JOIN mangadapdb.cleanspaxelprop7 ON mangadapdb.file.pk = mangadapdb.cleanspaxelprop7.file_pk JOIN mangadatadb.pipeline_info AS drpalias ON drpalias.pk = mangadatadb.cube.pipeline_info_pk JOIN mangadatadb.pipeline_info AS dapalias ON dapalias.pk = mangadapdb.file.pipeline_info_pk JOIN (SELECT mangadapdb.cleanspaxelprop7.file_pk AS binfile, count(mangadapdb.cleanspaxelprop7.pk) AS goodcount\n",
    "FROM mangadapdb.cleanspaxelprop7\n",
    "WHERE mangadapdb.cleanspaxelprop7.binid_binned_spectra != -1 AND mangadapdb.cleanspaxelprop7.binid_stellar_continua != -1 AND mangadapdb.cleanspaxelprop7.binid_spectral_indices != -1 AND mangadapdb.cleanspaxelprop7.binid_em_line_moments != -1 AND mangadapdb.cleanspaxelprop7.binid_em_line_models != -1 GROUP BY mangadapdb.cleanspaxelprop7.file_pk) AS bingood ON bingood.binfile = mangadapdb.cleanspaxelprop7.file_pk JOIN (SELECT mangadapdb.cleanspaxelprop7.file_pk AS valfile, count(mangadapdb.cleanspaxelprop7.pk) AS valcount\n",
    "FROM mangadapdb.cleanspaxelprop7\n",
    "WHERE mangadapdb.cleanspaxelprop7.emline_gflux_ha_6564 > 5 GROUP BY mangadapdb.cleanspaxelprop7.file_pk) AS goodhacount ON goodhacount.valfile = mangadapdb.cleanspaxelprop7.file_pk\n",
    "WHERE drpalias.pk = 32 AND dapalias.pk = 34 AND goodhacount.valcount >= 0.2 * bingood.goodcount) AS anon_1 GROUP BY anon_1.mangadatadb_cube_mangaid, anon_1.mangadatadb_cube_plate, concat(anon_1.mangadatadb_cube_plate, '-', anon_1.mangadatadb_ifudesign_name), anon_1.mangadatadb_ifudesign_name\n",
    "\n",
    "### Postgres Results\n",
    "Above postgres query time takes ~34 minutes to return 664 row results.\n",
    "\n",
    "1st run\n",
    "Time: 1998737.520 ms (33:18.738)\n",
    "\n",
    "2nd run\n",
    "Time: 2071235.639 ms (34:31.236)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark Query via DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define good spaxels\n",
    "good_spaxels = ((maps.binid_binned_spectra != -1) & \n",
    "                   (maps.binid_stellar_continua != -1) & \n",
    "                   (maps.binid_em_line_models != -1) & \n",
    "                   (maps.binid_em_line_moments != -1) & \n",
    "                   (maps.binid_spectral_indices != -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "664\n",
      "Elapsed time [sec]: 17.228862084448338"
     ]
    }
   ],
   "source": [
    "# run the PySpark query\n",
    "with Timer():\n",
    "    # get total counts of number of good spaxels, grouped by plateifu\n",
    "    tc = maps.filter(good_spaxels).groupby('plateifu').count().withColumnRenamed('count', 'totalc')\n",
    "\n",
    "    # get counts of number of good spaxels with H-alpha > 5, grouped by plateifu\n",
    "    fc = maps.filter(good_spaxels).filter(maps['emline_gflux_ha_6564'] > 5).groupby('plateifu').count().withColumnRenamed('count', 'filterc')\n",
    "\n",
    "    # join the tables and filter where \n",
    "    tmp = tc.join(fc, 'plateifu')\n",
    "    tmp.filter(tmp.filterc >= 0.2 * tmp.totalc).count()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark Query via Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "664\n",
      "Elapsed time [sec]: 146.5893799038604"
     ]
    }
   ],
   "source": [
    "# define sql command to count total number of good spaxels\n",
    "totsql = \"\"\" select f.plateifu, count(f.*) as tcount from maps as f \\\n",
    "where f.binid_binned_spectra != -1 and f.binid_stellar_continua != -1 and f.binid_em_line_models != -1 \\\n",
    "and f.binid_em_line_moments != -1 and f.binid_spectral_indices != -1 group by f.plateifu\n",
    "\"\"\"\n",
    "\n",
    "# define sql command to count number of good spaxels with H-alpha flux > 5\n",
    "hasql = \"\"\" select f.plateifu, count(f.*) as vcount from maps as f \\\n",
    "where f.emline_gflux_ha_6564 > 5 and f.binid_binned_spectra != -1 and f.binid_stellar_continua != -1 \\\n",
    "and f.binid_em_line_models != -1 and f.binid_em_line_moments != -1 and f.binid_spectral_indices != -1 \\\n",
    "group by f.plateifu\n",
    "\"\"\"\n",
    "\n",
    "# construct the complete sql command to select galaxies that have an H-alpha flux > 5 \n",
    "# in more than 20% of their spaxels \n",
    "sql = \"\"\" select t.plateifu, t.tcount, v.vcount from (select f.plateifu, count(f.*) as tcount from maps as f \\\n",
    "where f.binid_binned_spectra != -1 and f.binid_stellar_continua != -1 and f.binid_em_line_models != -1 \\\n",
    "and f.binid_em_line_moments != -1 and f.binid_spectral_indices != -1 group by f.plateifu) as t, (select f.plateifu, count(f.*) as vcount from maps as f \\\n",
    "where f.emline_gflux_ha_6564 > 5 and f.binid_binned_spectra != -1 and f.binid_stellar_continua != -1 \\\n",
    "and f.binid_em_line_models != -1 and f.binid_em_line_moments != -1 and f.binid_spectral_indices != -1 \\\n",
    "group by f.plateifu) as v where t.plateifu=v.plateifu and \\\n",
    "v.vcount >= 0.2*t.tcount\n",
    "\"\"\"\n",
    "\n",
    "with Timer():\n",
    "    spark.sql(sql).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Query 2\n",
    "Select all galaxies with an NSA sersic_n index < 2, an H-alpha summed-EW > 6, and an NSA sersic log stellar mass between 9.5-11\n",
    "\n",
    "### Raw SQL \n",
    "\n",
    "SELECT mangadatadb.cube.mangaid AS \"cube.mangaid\", mangadatadb.cube.plate AS \"cube.plate\", concat(mangadatadb.cube.plate, '-', mangadatadb.ifudesign.name) AS \"cube.plateifu\", mangadatadb.ifudesign.name AS \"ifu.name\", mangadapdb.cleanspaxelprop7.emline_sew_ha_6564 AS emline_sew_ha_6564, mangasampledb.nsa.sersic_n AS \"nsa.sersic_n\", CAST(CASE WHEN (mangasampledb.nsa.sersic_mass > 0.0) THEN log(mangasampledb.nsa.sersic_mass) WHEN (mangasampledb.nsa.sersic_mass = 0.0) THEN 0.0 END AS FLOAT) AS \"nsa.sersic_logmass\", mangadapdb.cleanspaxelprop7.x AS \"spaxelprop.x\", mangadapdb.cleanspaxelprop7.y AS \"spaxelprop.y\"\n",
    "FROM mangadatadb.cube JOIN mangadatadb.ifudesign ON mangadatadb.ifudesign.pk = mangadatadb.cube.ifudesign_pk JOIN mangadapdb.file ON mangadatadb.cube.pk = mangadapdb.file.cube_pk JOIN mangadapdb.cleanspaxelprop7 ON mangadapdb.file.pk = mangadapdb.cleanspaxelprop7.file_pk JOIN mangasampledb.manga_target ON mangasampledb.manga_target.pk = mangadatadb.cube.manga_target_pk JOIN mangasampledb.manga_target_to_nsa ON mangasampledb.manga_target.pk = mangasampledb.manga_target_to_nsa.manga_target_pk JOIN mangasampledb.nsa ON mangasampledb.nsa.pk = mangasampledb.manga_target_to_nsa.nsa_pk JOIN mangadatadb.pipeline_info AS drpalias ON drpalias.pk = mangadatadb.cube.pipeline_info_pk JOIN mangadatadb.pipeline_info AS dapalias ON dapalias.pk = mangadapdb.file.pipeline_info_pk\n",
    "WHERE CAST(CASE WHEN (mangasampledb.nsa.sersic_mass > 0.0) THEN log(mangasampledb.nsa.sersic_mass) WHEN (mangasampledb.nsa.sersic_mass = 0.0) THEN 0.0 END AS FLOAT) >= 9.5 AND CAST(CASE WHEN (mangasampledb.nsa.sersic_mass > 0.0) THEN log(mangasampledb.nsa.sersic_mass) WHEN (mangasampledb.nsa.sersic_mass = 0.0) THEN 0.0 END AS FLOAT) < 11.0 AND mangasampledb.nsa.sersic_n < 2.0 AND mangadapdb.cleanspaxelprop7.emline_sew_ha_6564 > 6.0 AND drpalias.pk = 32 AND dapalias.pk = 34\n",
    "\n",
    "### Postgres Results\n",
    "Above postgres query takes ~3 mins to run first time, ~15 seconds after query caching, returning 1,235,317 row results.\n",
    "\n",
    "1st run\n",
    "Time: 201567.467 ms (03:21.567)\n",
    "\n",
    "2nd run\n",
    "Time: 15866.201 ms (00:15.866)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark Query via DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1235317\n",
      "Elapsed time [sec]: 3.6993655618280172"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import log10\n",
    "with Timer():\n",
    "    # filter drpall on sersic index and log of stellar mass\n",
    "    sub = drpall.filter((drpall.nsa_sersic_n < 2.0) & \n",
    "                  (log10(drpall.nsa_sersic_mass) >=9.5) & \n",
    "                  (log10(drpall.nsa_sersic_mass) < 11.))\n",
    "\n",
    "    # filter the maps on h-alpha EW\n",
    "    ew = maps.filter(good_spaxels).filter(maps.emline_sew_ha_6564 > 6)\n",
    "\n",
    "    # join the tables and select some,count\n",
    "    tmp = ew.join(sub, 'plateifu')\n",
    "    tmp.select(tmp.plateifu, tmp.x, tmp.y, tmp.emline_sew_ha_6564, tmp.nsa_sersic_n, log10(tmp.nsa_sersic_mass)).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark Query via Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1235317\n",
      "Elapsed time [sec]: 3.1305666361004114"
     ]
    }
   ],
   "source": [
    "# create sql command\n",
    "sql = \"\"\"select f.plateifu, f.emline_sew_ha_6564, d.nsa_sersic_n, log10(d.nsa_sersic_mass) \\\n",
    "from maps as f join drpall as d on d.plateifu=f.plateifu \\\n",
    "where (f.emline_sew_ha_6564 > 6 and f.binid_binned_spectra != -1 and f.binid_stellar_continua != -1 and \\\n",
    "f.binid_em_line_models != -1 and f.binid_em_line_moments != -1 and f.binid_spectral_indices != -1 and \\\n",
    "d.nsa_sersic_n < 2.0 and log10(d.nsa_sersic_mass) between 9.5 and 11.0)\"\"\"\n",
    "\n",
    "# run Spark sql\n",
    "\n",
    "with Timer():\n",
    "    spark.sql(sql).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
